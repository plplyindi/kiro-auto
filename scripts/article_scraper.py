#!/usr/bin/env python3
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹çˆ¬å–å·¥å…·
ä»æ–‡ç« é“¾æ¥çˆ¬å–æ ‡é¢˜ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ã€æ­£æ–‡å†…å®¹ç­‰
"""

import requests
import re
import json
import os
from datetime import datetime
from urllib.parse import urlparse, parse_qs
import time

class WeChatArticleScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def scrape_article(self, url):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        try:
            print(f"ğŸ” æ­£åœ¨çˆ¬å–: {url[:80]}...")
            
            response = requests.get(url, headers=self.headers, timeout=30)
            response.encoding = 'utf-8'
            html = response.text
            
            # æå–æ–‡ç« ä¿¡æ¯
            article = {
                "url": url,
                "scraped_at": datetime.now().isoformat()
            }
            
            # æå–æ ‡é¢˜
            title_match = re.search(r'<h1[^>]*class="rich_media_title"[^>]*>(.*?)</h1>', html, re.DOTALL)
            if title_match:
                article["title"] = re.sub(r'<[^>]+>', '', title_match.group(1)).strip()
            else:
                # å°è¯•ä»metaæ ‡ç­¾æå–
                title_match = re.search(r'var msg_title = "(.*?)";', html)
                if title_match:
                    article["title"] = title_match.group(1).strip()
            
            # æå–ä½œè€…
            author_match = re.search(r'<span[^>]*class="rich_media_meta rich_media_meta_text"[^>]*>(.*?)</span>', html)
            if author_match:
                article["author"] = author_match.group(1).strip()
            else:
                # å°è¯•ä»JavaScriptå˜é‡æå–
                author_match = re.search(r'var nickname = "(.*?)";', html)
                if author_match:
                    article["author"] = author_match.group(1).strip()
            
            # æå–å‘å¸ƒæ—¶é—´
            time_match = re.search(r'var publish_time = "(\d+)"', html)
            if time_match:
                timestamp = int(time_match.group(1))
                article["publish_time"] = datetime.fromtimestamp(timestamp).isoformat()
            
            # æå–æ­£æ–‡å†…å®¹
            content_match = re.search(r'<div[^>]*class="rich_media_content[^"]*"[^>]*>(.*?)</div>', html, re.DOTALL)
            if content_match:
                content_html = content_match.group(1)
                # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬
                content_text = re.sub(r'<[^>]+>', '', content_html)
                # æ¸…ç†å¤šä½™ç©ºç™½
                content_text = re.sub(r'\s+', ' ', content_text).strip()
                article["content"] = content_text[:5000]  # ä¿ç•™å‰5000å­—ç¬¦
                article["content_length"] = len(content_text)
            
            # æå–æ‘˜è¦
            digest_match = re.search(r'var msg_desc = "(.*?)";', html)
            if digest_match:
                article["digest"] = digest_match.group(1).strip()
            
            print(f"âœ… æˆåŠŸçˆ¬å–: {article.get('title', 'Unknown Title')}")
            return article
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            return {
                "url": url,
                "error": str(e),
                "scraped_at": datetime.now().isoformat()
            }
    
    def scrape_from_links_file(self, links_file="articles_links.json", output_file="articles_content.json"):
        """ä»é“¾æ¥æ–‡ä»¶æ‰¹é‡çˆ¬å–æ–‡ç« """
        if not os.path.exists(links_file):
            print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {links_file}")
            return []
        
        # è¯»å–é“¾æ¥
        with open(links_file, 'r', encoding='utf-8') as f:
            links_data = json.load(f)
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(links_data)} ä¸ªæ–‡ç« é“¾æ¥")
        
        # è¯»å–å·²çˆ¬å–çš„æ–‡ç« ï¼ˆé¿å…é‡å¤çˆ¬å–ï¼‰
        existing_articles = {}
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    articles = json.load(f)
                    existing_articles = {a['url']: a for a in articles}
                print(f"ğŸ“š å·²æœ‰ {len(existing_articles)} ç¯‡æ–‡ç« ")
            except:
                pass
        
        # çˆ¬å–æ–°æ–‡ç« 
        new_count = 0
        for link_data in links_data:
            url = link_data['url']
            
            # è·³è¿‡å·²çˆ¬å–çš„æ–‡ç« 
            if url in existing_articles:
                print(f"â­ï¸  è·³è¿‡å·²çˆ¬å–: {existing_articles[url].get('title', url[:50])}")
                continue
            
            # çˆ¬å–æ–‡ç« 
            article = self.scrape_article(url)
            
            # åˆå¹¶é‚®ä»¶æ¥æºä¿¡æ¯
            article.update({
                "source_email": {
                    "subject": link_data.get("email_subject"),
                    "from": link_data.get("email_from"),
                    "date": link_data.get("email_date")
                }
            })
            
            existing_articles[url] = article
            new_count += 1
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        # ä¿å­˜æ‰€æœ‰æ–‡ç« 
        all_articles = list(existing_articles.values())
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_articles, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
        print(f"   æ–°å¢: {new_count} ç¯‡")
        print(f"   æ€»è®¡: {len(all_articles)} ç¯‡")
        print(f"   ä¿å­˜åˆ°: {output_file}")
        
        return all_articles

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("  å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹çˆ¬å–å·¥å…·")
    print("=" * 50)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("")
    
    scraper = WeChatArticleScraper()
    articles = scraper.scrape_from_links_file()
    
    if articles:
        # ç”Ÿæˆç®€å•çš„Markdownæ‘˜è¦
        summary_file = "articles_summary.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("# å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ±‡æ€»\n\n")
            f.write(f"æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"æ€»è®¡: {len(articles)} ç¯‡æ–‡ç« \n\n")
            f.write("---\n\n")
            
            for i, article in enumerate(articles, 1):
                f.write(f"## {i}. {article.get('title', 'æ— æ ‡é¢˜')}\n\n")
                f.write(f"- **ä½œè€…**: {article.get('author', 'æœªçŸ¥')}\n")
                if article.get('publish_time'):
                    f.write(f"- **å‘å¸ƒæ—¶é—´**: {article['publish_time']}\n")
                if article.get('digest'):
                    f.write(f"- **æ‘˜è¦**: {article['digest']}\n")
                f.write(f"- **é“¾æ¥**: [æŸ¥çœ‹åŸæ–‡]({article['url']})\n")
                if article.get('content_length'):
                    f.write(f"- **å­—æ•°**: {article['content_length']} å­—\n")
                f.write("\n")
                
                if article.get('content'):
                    preview = article['content'][:200]
                    f.write(f"**å†…å®¹é¢„è§ˆ**:\n\n{preview}...\n\n")
                
                f.write("---\n\n")
        
        print(f"ğŸ“ ç”Ÿæˆæ–‡ç« æ‘˜è¦: {summary_file}")
    
    print("")
    print("=" * 50)
    print(f"âœ… ä»»åŠ¡å®Œæˆ")
    print(f"â° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)
    
    return 0

if __name__ == "__main__":
    exit(main())
